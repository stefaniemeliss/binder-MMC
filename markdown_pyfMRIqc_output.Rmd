---
title: "Summary pyfMRIqc output"
author: "Stef Meliss"
date: "30/03/2021"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
#### setups ####
#empty work space, load libraries and functions
rm(list=ls())

# load libraries
library(dplyr)
library(ggplot2)
library(ggrepel)
library(grid)

# function to determine outliers
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

### downlaod data from OSF ###

project <- osfr::osf_retrieve_node("eyzwb")

osfr::osf_ls_files(project, pattern = "pyfMRIqc.csv") %>%
  osfr::osf_download(conflicts = "overwrite")

# load in dataset
scanparam <- read.csv("data_pyfMRIqc.csv")

# create ID column
scanparam$ID <- gsub("sub-control0", "", scanparam$subject)
scanparam$ID <- gsub("sub-experimental0", "", scanparam$ID)
#scanparam$ID <- as.numeric(scanparam$ID)

# remove scientific notation
scanparam$value <-format(scanparam$value, scientific = FALSE) 
scanparam$value <-as.numeric(scanparam$value) 

# make subject a character variable
scanparam$subject <- as.character(scanparam$subject)

# eliminate acq in BOLD
scanparam$acq <- gsub("_acq-1", "", scanparam$BOLD)
scanparam$acq <- gsub("_acq-2", "", scanparam$acq)

# shorten acq
scanparam$acq <- gsub("magictrickwatching", "magictricks", scanparam$acq)

# determine DV to analyse
DV_plot <- c("Mean", "Mean_(mask)", "SD", "SD_(mask)", 
             "SNR_voxel_MEAN", "SNR_voxel_STD", 
             "Min_Slice_SNR", "Max_Slice_SNR", "Mean_voxel_SNR", 
             "Mean_absolute_Movement", "Max_absolute_Movement", 
             "Max_relative_Movement", "Relative_movements_(>0.1mm)", "Relative_movements_(>0.5mm)",
             "volumes", "threshold_02", "fraction_02", 
             "threshold_03", "fraction_03", 
             "threshold_05", "fraction_05",
             "threshold_1", "fraction_1") 

definitions <- c(
  "Mean signal (unmasked), i.e. mean voxel (all voxels) intensity over time using **all** voxels. The tool also provides a 2D png image (axial slices) as output for the mean voxel intensity. It can be used to see if there is any general signal loss.",
  
  "Mean signal (masked), i.e. mean voxel intensity over time using **voxels inside the brain mask** only.",
  
  "Standard deviation of mean signal (unmasked), i.e. of voxel intensity over time using **all** voxels. The tool additionally outputs a 2D png image (axial slices). By default, the grey matter, brain stem and blood vessels have the highest variability of BOLD signal. Therefore, these tissue types have higher values and are brighter in this image. Sudden or unexpected signal changes such as artefacts, signal changes, motion etc. in one or a few volumes will increase the variance and therefore will be detectable in these images.",
  
  "Standard deviation of mean signal (masked), i.e. of voxel intensity over time using **voxels inside the brain mask** only.",
  
  "Mean values of voxel for SNR (these are the voxels highlighted in green on the masks image, so that this is the mean voxel intensity over time of the voxels outside the brain mask and below SNR threshold, i.e. noise).",
  
  "Standard deviation of voxel for SNR (these are the voxels highlighted in green on the masks image, so that this is the standard deviation of voxel intensity over time of the voxels outside the brain mask and below SNR threshold, i.e. noise).",
  
  "Lowest slice SNR value (within the brain mask). The slice SNRs measures the time course SNR averaged across each slice. The higher the SNR, the smaller the relative fluctuations and more stable is the signal over repeated measurements.",
  
  "Highest slice SNR value (within the brain mask). The slice SNRs measures the time course SNR averaged across each slice. The higher the SNR, the smaller the relative fluctuations and more stable is the signal over repeated measurements.",
  
  "Mean voxel SNR is the average over all the slices together",
  
  "Absolute movement captures the difference in relation to the base volume. The values have been transformed into absolute values before calculating the mean. The mean has been calculated over all volumens and across all 6 motion parameters.",
  
  "Maximum absolute movement (in absolute values) across all volumens and motion parameters.",
  
  "Relative movement captures the difference to the previous volume (rather than to the base volume). Maximum value of relative movement (in absolute values) across all volumens and motion parameters.",
  
  "Absolute values of relative movement are compared against threshold of 0.1mm. Output captures number of movements, rather than number of slices affected.",
  
  "Absolute values of relative movement are compared against threshold of 0.5mm. Output captures number of movements, rather than number of slices affected.",
  
  "Volumes captures the number of volumes each scan has and has been determined using AFNI's *3dinfo*. Because the magictricks vary in length and were displayed pseudo-randomly across blocks, the number of volumes shows somw variation.",
  
  "Using the method described above, this parameter shows the absolute number of volumes above the motion limit of 0.2. This is the default for Framewise Displacement (sum of relative movements across all 6 parameters) in MRIQC, however, the Euclidian distance used here is slightly different from that and can only be seen as a proxy.",
  
  "Using the method described above, this parameter shows the percentage of volumes above the motion limit of 0.2.",
  
  "Using the method described above, this parameter shows the absolute number of volumes above the motion limit of 0.3. This is the threshold I used in my pre-processing pipeline to censor motion.",
  
  "Using the method described above, this parameter shows the percentage of volumes above the motion limit of 0.3.",
  
  "Using the method described above, this parameter shows the absolute number of volumes above the motion limit of 0.5. This is closer to what was suggested in Powers et al (2012) though they use Framewise Displacement rather than the Euclidean norm.",
  
  "Using the method described above, this parameter shows the percentage of volumes above the motion limit of 0.5.",
  
  "Using the method described above, this parameter shows the absolute number of volumes above the motion limit of 1.",
  
  "Using the method described above, this parameter shows the percentage of volumes above the motion limit of 1.")

interpretation <- c(
  "Mean voxel intensity across all voxels is fairly normally distributed across the sample as well as within each task and group. The mean and SD are also comparable. The outliers sub-control003 and sub-experimental014 have been screened in more detail, but the mean image does not suggest any signal loss for either of them. sub-experimental014 has a large head which could have contributed to high mean values. The spaghetti plot suggests that the mean voxel intensity across time is similar across scans, so that there is no evidence scanner issues during the course of an experiment. This is also reflected by a low mean and SD of the subjectwise SD, i.e. the SD of the values for each subject.",
  
  "Results observed in the mean voxel intensity across all voxels are mirrored in the mean voxel intensity within the brain mask. When only looking at brain voxels, the mean intensity increases and is not affected by task or group. Again, the values are fairly constant across the duration of each experiment. This is also reflected by a low mean and SD of the subjectwise SD, i.e. the SD of the values for each subject.",
  
  "The SD of voxel intensity across time for all voxels follows a normal distribution and does not differ across tasks and groups. The values are constant across scans for each subject, reflected by low mean and SD of the subjectwise SD. Again, sub-control003 and sub-experimental014 are outliers, but examining their variance images does not flag up potential artefacts.",
  
  "The SD of voxel intensity for voxels within the brain mask reflects the pattern observed previously. There are no differences depending on group and task and the values are constant within each subject.",
  
  "The mean of the voxels used for SNR calculation do not differ depending on group or task. sub-experimental018 values are outliers. After looking at the SNR mask (i.e. a mask showing with values have been used for the SNR calculation), this is due to the fact that a high number of voxels was classified as noise, some including skull and other non-brain tissues. Again, the values are constant across the scans for each subject.",
  
  "The SD of the voxels for SNR calculation does not differ depending on group or task. Again, sub-experimental018 is an outlier, but this seems to be due to the extend of the mask. The values are again constant across scans for each subject.",
  
  "The lowest slice SNR value is slightly lower in the control group compared to the experimental group, but is comparable for both tasks. Some subjects have fairly high values as their lowest SNR slice (i.e. sub-experimental034, sub-experimental028, sub-control015, sub-control041). The min slice SNR shows some variability across subjects, but this could potentially be attributed to the fact that participants might have moved in the breaks between the scans, so that a slice does not necessarily cover the same brain regions across scans.",
  
  "The hightest slice SNR is slightly higher in the control group compared to the experimental group, but is comparable for both tasks. sub-experimental034 is an outlier. The max slice SNR values are more constant across scans for each subject than the min slice SNR values.", 
  
  "The SNR averaged across slices follows a normal distribution and is similar across tasks and groups though the experimental group has higher SDs. Again, sub-experimental034 is an outlier. The values are constant across scans for each subject.",
  
  "The mean absolute movement is constant across tasks and groups. Some subjects show a high value, either in the first task run or first rest run. There is considerable heterogeneity across scans showing that the motion was not constant across the duration of the experiment.", 
  
  "The max absolute movement values are unfortunately quite high, some even exceeding voxel size. This affects both tasks and both groups. Again, there is a lot of variation across the experiment for each subject.",
  
  "The max relative movement values are luckily smaller and do not exceed voxelsize. Outliers can be found across groups and tasks. As previously, movement changes throughout the course of the experiment.",
  
  "When interpreting these numbers, it needs to be considered that the reflect the number of movements in any of the 6 slices rather than the number of volumes affected. Some subjects have a high number of movements > 0.1mm, but again, both groups and tasks are affected. Again, there are changes throughout the duration of the experimentn and it seems that people were moving more as the experiment proceeded.", 
  
  "When looking at relative movements > 0.5mm, luckily the average number drops and again, the number reflects movements in any of the 6 parameters rather than slices affected. Both tasks and groups are affected and values change throughout the experiment.",
  
  "Volumes for resting state are constant at 300. For the task blocks, they sum up to 1140, but show some variation. The values are the same for both groups though as each pseudo-randomised trial order was used once within the reward group and once within the control group. The only difference there is is caused by the fact that the scanner needed to be restarted for one subject (sub-experimental016). Average volumes per slice are comparable across task blocks.",
  
  "The average number of volumes above the 0.2 movement threshold is moderately high, but shows considerable variation within the sample as well as depending on group and task. Additionally, similarily to the other motion parameters, there is a lot of variation within each subject across the scans.",
  
  "When looking at the percentage of volumes above 0.2 motion, the average is 7% (though the median is lower and the mean is influenced by outliers). We see some strong outliers across tasks and groups (some up to 50%). Again, there is considerable variation across the experiment and subjects.",
  
  "As expected, the absolute numbers are lower for the 0.3 threshold compared to the previous one. The number of outliers also decreases, but the same extreme outliers can still be identified.",
  
  "Mean drops to 3% when looking at the percentage of volumes above the 0.3 threshold. Additionally, this threshold seems to be less influenced by group and task.",
  
  "Again, the absolute number of volumes above the 0.5 threshold drops further in comparison to the previous threshold, but is still influenced by outliers in both groups and tasks and varies over the course of the experiment and across subjects. Interestingly, there seems to be less head motion during resting state compared to the task.",
  
  "The percentage of volumes above 0.5 motion drops to around 1%, but there are subjects across both tasks and groups with up to 20% volumes affected.",
  
  "Only a very small number of volumes have motion above the threshold of 1 and most subjects do not exceed this threshold at all. Those that do are labelled as outliers.",
  
  "On average, there are 0% of volumes affected by motion above 1mm, but some scans are affected by this with up to 5%.")


# determine font sizes
text_size <- 8
axis_text_size <- 10
axis_title_size <- 12
title_size <- 14
strip_text_size <- 12

```

This markdown summarises the output from pyfMRIqc (https://github.com/DrMichaelLindner/pyfMRIqc), a tool develeoped by Michael Lindner & Brendan Williams. The toolbox takes 4D timeseries and computes different QC parameters. Raw nifti files were used as imput, however, the time series acquired during the magic trick watching task was cut so that it only includes volumes covering the task. This was necessary because the scanner was stopped manually at the end of the task.  
  
All images share the following scan paramters: 

* Slice Resolution: 64x64
* Num slices: 37
* Voxel Size: 3.0x3.0x3.75
* TR = 2000ms
* Num Volumes: rest = 300 (each), magictricks = 1140 (in total)  

Additionally, a mask was used for signal-to-noise (SNR) calculations. The mask was created for each timeseries seperately using AFNI's *3dAutomask* and 25 was used as SNR threshold, i.e. percentage of voxel with the lowest values (outside the mask) for SNR calculation (scalar value). SNR is calculated as mean voxel intensity over time (within the brain mask) divided by standard deviation (SD) of the mean of noise. Mean noise is defined as average of the 25% (i.e. SNR threshold) of voxel having the lowest mean intensity (outside the brain mask). The SNR provides an estimate of the reliability (~ reproducibility) of fMRI data and serves as a general goodness measure.

Motion parameters have been calculated using AFNI's *3dvolreg*. This file containes the absolute motion relative to the first volume. Relative motion was then calculated with pyfMRIqc. 


In total, there are the following **output metrics**:

* Mean
* Mean_(mask)
* SD
* SD_(mask)
* SNR_voxel_MEAN
* SNR_voxel_STD
* Min_Slice_SNR
* Max_Slice_SNR
* Mean_voxel_SNR
* Mean_absolute_Movement
* Max_absolute_Movement
* Max_relative_Movement
* Relative_movements_(>0.1mm)
* Relative_movements_(>0.5mm)
* Relative_movements_(>voxelsize) - **There was no relative movement > voxelsize, the output was hence omitted.**

Additionally, the following parameters were added to further investigate motion:

* volumes  
* threshold_02  
* fraction_02  
* threshold_03  
* fraction_03  
* threshold_05  
* fraction_05  
* threshold_1  
* fraction_1  

threshold/fraction capture the absolute/relative **number of volumes** above a given threshold. pyfMRIqc, in comparison, captures the number of movements across all 6 motion parameters that exceed a given threshold. To determine number/percentage of volumes affected by motion above a certain threshold, AFNI's *1d_tool.py -infile $motionfile_prefix -derivative -demean -collapse_cols euclidean_norm -verb 0 -show_censor_count -moderate_mask 0 THRESHOLD* was used.  

This program uses the motion file $motionfile_prefix created using *3dvolreg*, computes the temporal derivative of each vector (done as first backward difference): "Take the backward differences at each time point.  For each index > 0, value[index] = value[index] - value[index-1], and value[0] = 0", that is, it calculates the relative motion and demeans that. Additionally, the columns (i.e. the 6 motion parameters) are then collapsed using "euclidean_norm" method (sqrt(sum squares)). This combines rotations (in degrees) with shifts (in mm) as if they had the same weight:

"Note that assuming rotations are about the center of mass (which should produce a minimum average distance), then the average arc length (averaged over the brain mask) of a voxel rotated by 1 degree (about the CM) is the following (for the given datasets):  
TT_N27+tlrc:        0.967 mm (average radius = 55.43 mm)  
MNIa_caez_N27+tlrc: 1.042 mm (average radius = 59.69 mm)  
MNI_avg152T1+tlrc:  1.088 mm (average radius = 62.32 mm)  
The point of these numbers is to suggest that equating degrees and mm should be fine. The average distance caused by a 1 degree rotation is very close to 1 mm (in an adult human)."  
[Please note at this point that pyfMRIqc assumes a 50mm head radius in their translation from radians to mm.]  

The Euclidean norm of the relative motion is then compared against the defined THRESHOLDS (i.e. 0.2, 0.3, 0.5 and 1) and outliers (in volumes) are counted, i.e. if >= limit or <= limit, result is 1.  
  
Below, each of these metrics will be defined and interpreted. Additionally, for each of these metrics, **four graphs** have been created to explore them in the sample:  

a) Histogram for all scans  
Data from control and experimental group is shown in dark and light gray, respectively. Moreover, mean and median are added using vertical lines in red and green and their values together with their variance (standard deviation (SD) and median absolute deviation (MAD)) are annotated in the corresponding colour.  
b) Histogram divided by group (control vs experimental) and task (magictricks vs rest)  
Similar to the histogram for all scans, mean and median and respective measures of variance are highlighted seperatly for each cell. Additionally, effects for group and task are tested inference-statistically using a linear mixed-effects model predicting the given metric using task and group while specifying random intercepts for subject and acquisition (metrics ~ group + task + (1 | ID) + (1 | acqusition)).  
c) Boxplot for each acquisition (magictricks_run-1 vs. magictricks_run-2 vs. magictricks_run-3 vs. rest_run-1 vs. rest_run-2)  
In the boxplots, outliers are determined as: value < quantile(value, 0.25) - 1.5 * IQR(value) | value > quantile(value, 0.75) + 1.5 * IQR(value). If a value is regarded as outlier, it is using the subject's ID. The labels are printed in black if this subject belongs to the control group and in grey if the subject belongs to the experimental group.
d) Spaghettiplot highlighting the time course across the experiment of each of these metrics for each subject  
In the spaghetti plot, a subject is labeled if any of the values over time were larger than mean(value) + 1SD(value). Please note that the line for subject 16 makes a little dip for magictricks_run-2. This is because the scanner had to be stopped during the second block, so there are two acqusitions (both covering approximately half of the block). To provide summary statistics, the IQM was standardised across the sample before computing mean, standard deviation, and range within each subject.  


```{r plots, echo=FALSE, message=FALSE, warning=FALSE,  fig.align = "center", out.width = '100%', fig.width = 11, results="asis"}
i <- 0
# loop over dependent variables to create plots 
for (DV in DV_plot){
  
  # create data frame
  output <- subset(scanparam, scanparam$param == paste0(DV))
  
  # compute LMER model
  model <- lme4::lmer(value ~ group + task + (1 | ID) + (1 | acq), data = output)
  
  i <- i + 1
  # print out DV to Markdown
  cat("  \n### Summary", DV)
  cat("  \n")
  cat("  \n Definition:", definitions[i])
  cat("  \n")
  cat("  \n Interpretation:", interpretation[i])
  cat("  \n")
  
  # mean = red line; median = green line
  # Create a text
  Mean <- grobTree(textGrob(paste0("Mean = ", round(mean(output$value),2), " (SD = ", round(sd(output$value),2),")"), x=0.6,  y=0.9, hjust=0,
                            gp=gpar(col="red", fontsize=13, fontface="italic")))
  Median <- grobTree(textGrob(paste0("Median = ", round(median(output$value),3), " (MAD = ", round(mad(output$value),3),")"), x=0.6,  y=0.85, hjust=0,
                              gp=gpar(col="green", fontsize=13, fontface="italic")))
  # (a) histogram for all 250 scans
  all <- ggplot(output, aes(x=value, col = group, fill = group)) + 
    geom_histogram(position = "stack", alpha=0.2, col = "black") +
    geom_vline(aes(xintercept = mean(value)),col='red', size=1) + 
    geom_vline(aes(xintercept = median(value)),col='green', size=1) + 
    theme_classic() +  scale_colour_grey() + scale_fill_grey() +
    labs(x=paste(DV), y="Frequency", title = "Histogram all scans") +
    theme(axis.text=element_text(size=axis_text_size), axis.title=element_text(size=axis_title_size, face="bold"), title=element_text(size =title_size, face="bold"), strip.text = element_text(size = strip_text_size)) 
  
  # combine graph and annotations
  all <- all + annotation_custom(Mean) + annotation_custom(Median)
  cat("  \n")
  print(all)
  cat("  \n")
  
  # (b) histogram splitted by task and group
  # mean = red line; median = green line
  task <- ggplot(output, aes(x=value)) +
    theme_classic() +  #scale_colour_grey() +
    geom_histogram(position="dodge", alpha=0.2, col = "black") +
    geom_vline(data = plyr::ddply(output, c("task", "group"), summarize, mean = mean(value, na.rm = T)), aes(xintercept=mean), col='red') +
    geom_vline(data = plyr::ddply(output, c("task", "group"), summarize, median = median(value, na.rm = T)), aes(xintercept=median), col='green') +
    labs(x=paste(DV), y="Frequency", title = "Histogram scans split by task and group") +
    theme(axis.text=element_text(size=axis_text_size), axis.title=element_text(size=axis_title_size, face="bold"), title=element_text(size =title_size, face="bold"), strip.text = element_text(size = strip_text_size)) +
    facet_grid(group ~ task)
  # add mean and median as values
  #xpos <- 0.6*(min(output$value) + max(output$value))
  #xpos <- 0.8*max(output$value)
  xpos <- ggplot_build(task)$layout$panel_scales_x[[1]]$range$range
  xrange <- max(xpos) - min(xpos)
  xpos <-  min(xpos) + .7 *xrange
  ypos1 <- max(ggplot_build(task)$data[[1]]$count) - sd(ggplot_build(task)$data[[1]]$count)
  ypos2 <- ypos1 - 0.5*sd(ggplot_build(task)$data[[1]]$count)
  ypos3 <- ypos2 - 0.5*sd(ggplot_build(task)$data[[1]]$count)
  ypos4 <- ypos3 - 0.5*sd(ggplot_build(task)$data[[1]]$count)
  task <- task +
    geom_text(data = plyr::ddply(output, c("task", "group"), summarize, mean = round(mean(value, na.rm = T), 3)),
              aes(label=paste("Mean =", mean), x = xpos, y = ypos1), vjust = 0, hjust = 0, col='red') +
    geom_text(data = plyr::ddply(output, c("task", "group"), summarize, sd = round(sd(value, na.rm = T), 3)),
              aes(label=paste0("(SD = ", sd, ")"), x = xpos, y = ypos2), vjust = 0, hjust = 0, col='red') +
    geom_text(data = plyr::ddply(output, c("task", "group"), summarize, median = round(median(value, na.rm = T), 3)),
              aes(label=paste("Median =", median), x = xpos, y = ypos3), vjust = 0, hjust = 0, col='green') +
    geom_text(data = plyr::ddply(output, c("task", "group"), summarize, mad = round(mad(value, na.rm = T), 3)),
              aes(label=paste0("(MAD = ", mad, ")"), x = xpos, y = ypos4), vjust = 0, hjust = 0, col='green')
  cat("  \n")
  print(task)
  
  cat("Output of Linear Mixed Effects model predicting QC metrics using group and task specifying random intercepts for subject and BOLD acquisition  \n")
  cat(sjPlot::tab_model(model, digits.p = 5)$knitr,"\n--------\n")  
  cat("  \n")
  
  
  # create label for outliers in boxplot
  # outlier = x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x)
  output_outliers <- output %>%
    group_by(acq) %>%
    mutate(outlier = ifelse(is_outlier(value), ID, NA))
  # (c) create boxplot for each acquisition
  boxplot <- ggplot(output_outliers, aes(y = value, x = acq, label = outlier)) +
    geom_boxplot()  + #geom_jitter(color = "red") +
    geom_label_repel(aes(label = outlier, color = group), nudge_x = 0.15, direction = "y", hjust = 0, segment.size = 0.2) +
    theme_classic() + scale_colour_grey() + theme(legend.position="none") +
    labs(y=paste(DV), x="BOLD acquisition", title = "Boxplot for each acquisition") +
    theme(axis.text=element_text(size=axis_text_size), axis.title=element_text(size=axis_title_size, face="bold"), title=element_text(size =title_size, face="bold"), strip.text = element_text(size = strip_text_size))
  cat("  \n")
  print(boxplot)
  cat("  \n")
  
  # creeate label for spaghetti plot
  # line gets labeled if  parameter > mean + 1SD
  output$above <- ifelse((mean(output$value, na.rm = T) + sd(output$value, na.rm = T)) < output$value, output$ID, NA) # add ID in column if value > mean + 1SD
  output$unique <- output$ID %in% unique(output$above) # determine unique subject names
  output$test <- ifelse(output$unique & output$acq == "rest_run-2", output$ID, NA) # add label to last acq only
  # (d) create spaghetti plot for each subject
  subj <- ggplot(output, aes(x=acq, y = value, group=subject, col = ID)) + geom_line() +
    facet_grid(group ~ .) +
    theme_classic() + theme(legend.position="none") +
    scale_x_discrete(limits=c("rest_run-1", "magictricks_run-1", "magictricks_run-2", "magictricks_run-3", "rest_run-2")) +
    labs(y=paste(DV), x="BOLD acquisition", title = "Parameters over time per subject") +
    theme(axis.text=element_text(size=axis_text_size), axis.title=element_text(size=axis_title_size, face="bold"), title=element_text(size =title_size, face="bold"), strip.text = element_text(size = strip_text_size)) +
    geom_label_repel(aes(label = test), nudge_x = 1, na.rm = TRUE, label.size = 0.05)
  cat("  \n")
  print(subj)
  cat("  \n")
  
  # calculate the mean value and SD of the parameter for each subject
  output$value_z <- scale(output$value, center = T, scale = T)
  per_subject <- plyr::ddply(output, "subject", summarize, mean = mean(value_z, na.rm = T), sd = sd(value_z, na.rm = T), min = min(value_z, na.rm = T), max = max(value_z, na.rm = T), range = max-min)

  # calculate subject-wise summary statistics
  mean <- psych::describe(per_subject$mean)
  mean$vars <- "Subject-wise mean"
  row.names(mean) <- NULL
  sd <- psych::describe(per_subject$sd)
  sd$vars <- "Subject-wise SD"
  row.names(sd) <- NULL
  range <- psych::describe(per_subject$range)
  range$vars <- "Subject-wise range"
  row.names(range) <- NULL

  # combine dataframes
  sum <- rbind(mean, sd)
  sum <- rbind(sum, range)
  
  if (i == 1){
    sd$vars <- paste("Subject-wise SD", DV)
    sd_all <- sd
  } else {
    sd$vars <- paste("Subject-wise SD", DV)
    sd_all <- rbind(sd_all, sd)
  }

  # print to markdown
  print(knitr::kable(sum, caption = paste("Subject-wise summary statistics of standardised", DV) ))
  cat('\n\n<!-- -->\n\n')
}
```
